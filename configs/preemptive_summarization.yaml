# =============================================================================
# Context Gateway - Production Configuration
# =============================================================================
# Preemptive Summarization: Background summarization for instant compaction.
# When context reaches 85%, a summary is generated in the background.
# When user calls /compact, the summary is already ready - zero wait time.
#
# This is the only enabled feature in the current release.
# =============================================================================

metadata:
  name: "Preemptive Summarization"
  description: "Background summarization for instant compaction"
  strategy: "passthrough"

server:
  port: ${GATEWAY_PORT:-18080}
  read_timeout: 30s
  # Long timeout for LLM responses with extended thinking (can take 5+ min)
  write_timeout: 1000s

urls:
  gateway: "http://localhost:${GATEWAY_PORT:-18080}"

# =============================================================================
# PROVIDERS - Define LLM providers once, reference everywhere
# =============================================================================
# Endpoints are auto-resolved from provider name + model:
#   - anthropic: api.anthropic.com/v1/messages
#   - gemini:    generativelanguage.googleapis.com/v1beta/models/{model}:generateContent
#   - openai:    api.openai.com/v1/chat/completions
#
# Example with multiple providers:
#   providers:
#     anthropic:
#       api_key: "${ANTHROPIC_API_KEY}"
#       model: "claude-haiku-4-5"
#     gemini:
#       api_key: "${GEMINI_API_KEY}"
#       model: "gemini-2.0-flash"
#     openai:
#       api_key: "${OPENAI_API_KEY}"
#       model: "gpt-4o-mini"
#
# Then reference in preemptive and pipes:
#   preemptive:
#     summarizer:
#       provider: "gemini"   # Use Gemini for summarization
#   pipes:
#     tool_output:
#       provider: "openai"   # Use OpenAI for compression
# =============================================================================

providers:
  anthropic:
    api_key: "${ANTHROPIC_API_KEY:-}"
    model: "claude-haiku-4-5"
  # Uncomment to use Gemini instead:
  # gemini:
  #   api_key: "${GEMINI_API_KEY}"
  #   model: "gemini-3-flash"

# =============================================================================
# AWS BEDROCK (opt-in, disabled by default)
# =============================================================================
# Enable to support AWS Bedrock as a provider with SigV4 signing.
# Requires AWS credentials (env vars, AWS_PROFILE, or IAM role).
#
# bedrock:
#   enabled: true

# =============================================================================
# PREEMPTIVE SUMMARIZATION
# =============================================================================

preemptive:
  enabled: true
  
  # Trigger when context usage reaches this percentage
  # 15% = triggers early (for testing), 85% = triggers when context is 85% full (production)
  trigger_threshold: 85.0
  
  # Add response headers for debugging (X-Context-Usage, X-Summary-Ready, etc.)
  add_response_headers: true
  
  # Logging
  log_dir: "${SESSION_DIR:-logs}"
  compaction_log_path: "${SESSION_COMPACTION_LOG:-logs/compaction.jsonl}"
  
  summarizer:
    # Option 1: Use a provider reference (recommended)
    provider: "anthropic"

    # Option 2: Inline settings (legacy, still supported)
    # model: "claude-haiku-4-5"
    # api_key: "${ANTHROPIC_API_KEY:-}"
    # endpoint: "https://api.anthropic.com/v1/messages"
    #
    # Option 3: AWS Bedrock (uses AWS credentials from environment)
    #   provider: "bedrock"
    #   model: "us.anthropic.claude-haiku-4-5-20251001-v1:0"
    #   endpoint: "https://bedrock-runtime.${AWS_REGION:-us-east-1}.amazonaws.com/model/us.anthropic.claude-haiku-4-5-20251001-v1:0/invoke"
    
    max_tokens: 4096
    timeout: 60s
    token_estimate_ratio: 4
  
  session:
    summary_ttl: 3h
    hash_message_count: 3

# =============================================================================
# COMPRESSION PIPES - Disabled (preemptive only)
# =============================================================================

pipes:
  tool_output:
    enabled: false
  tool_discovery:
    enabled: false

# =============================================================================
# STORE
# =============================================================================

store:
  type: "memory"
  ttl: 1h

# =============================================================================
# MONITORING
# =============================================================================

monitoring:
  log_level: "info"
  log_format: "console"
  log_output: "stdout"
  telemetry_enabled: true
  telemetry_path: "${SESSION_TELEMETRY_LOG:-logs/telemetry.jsonl}"
  compression_log_path: "${SESSION_COMPRESSION_LOG:-logs/compression.jsonl}"
